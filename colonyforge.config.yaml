# =============================================================================
# ColonyForge 設定ファイル
# =============================================================================
#
# このファイルはColonyForgeの動作を制御します。
# 環境変数で上書き可能: COLONYFORGE_<セクション>__<キー> (例: COLONYFORGE_SERVER__PORT=9000)
#
# ドキュメント: docs/ARCHITECTURE.md

# -----------------------------------------------------------------------------
# Hive基本設定
# -----------------------------------------------------------------------------
hive:
  name: "poc-project"        # プロジェクト名（Hive識別用）
  vault_path: "./Vault"      # イベントログ保存ディレクトリ

# -----------------------------------------------------------------------------
# ガバナンス設定
# エージェントの動作制限とタイムアウト
# -----------------------------------------------------------------------------
governance:
  max_retries: 3                    # タスク失敗時の最大リトライ回数
  max_oscillations: 5               # 状態振動（ループ）検知しきい値
  max_concurrent_tasks: 10          # 同時実行可能なタスク数
  task_timeout_seconds: 300         # タスクタイムアウト（秒）
  heartbeat_interval_seconds: 30    # ハートビート間隔（秒）
  approval_timeout_hours: 24        # ユーザー承認待ちタイムアウト（時間）
  archive_after_days: 7             # 完了Runをアーカイブするまでの日数

# -----------------------------------------------------------------------------
# LLM設定
# 使用するLLMプロバイダーとモデル
# LiteLLM SDK経由で100+プロバイダーを統一インターフェースで呼び出す
# サポートプロバイダー: openai, azure, anthropic, ollama, ollama_chat,
#   bedrock, vertex_ai, openrouter, huggingface, together_ai, groq, deepseek, litellm_proxy
# -----------------------------------------------------------------------------
llm:
  provider: "openai"              # プロバイダー名（上記リストから選択）
  model: "gpt-4o"                 # 使用モデル名
  api_key_env: "OPENAI_API_KEY"   # APIキーを格納する環境変数名
  max_tokens: 4096                # 最大トークン数
  temperature: 0.2                # 生成温度（0.0-2.0、低いほど決定的）
  # api_base: ""                  # カスタムAPIエンドポイント（Ollama, LiteLLM Proxy等）
  # num_retries: 3                # LiteLLMリトライ回数（429/5xx時）
  # fallback_models: []           # フォールバック先モデル（例: ["anthropic/claude-3-haiku-20240307"]）

  # レートリミット設定
  rate_limit:
    requests_per_minute: 60       # 1分あたりの最大リクエスト数
    requests_per_day: 0           # 1日あたりの最大リクエスト数（0=無制限）
    tokens_per_minute: 90000      # 1分あたりの最大LLMトークン数
    max_concurrent: 10            # 最大同時リクエスト数
    burst_limit: 10               # バースト許容数（瞬間的に許容するリクエスト数）
    retry_after_429: 60           # 429エラー時のデフォルト待機秒数

# --- Ollama（ローカルLLM）設定例 ---
# llm:
#   provider: "ollama_chat"         # Ollama chat API（ツール呼び出し対応）
#   model: "qwen3-coder"            # Ollamaにインストール済みのモデル名
#   api_base: "http://localhost:11434"  # OllamaサーバーURL
#   max_tokens: 4096
#   temperature: 0.2
#   # api_key_env は不要（ローカルモデルのため）
#   # num_retries: 3

# --- LiteLLM Proxy設定例 ---
# llm:
#   provider: "litellm_proxy"       # LiteLLM Proxyサーバー経由
#   model: "my-custom-model"        # Proxyに登録済みのモデル名
#   api_base: "http://litellm-proxy:4000"  # ProxyサーバーURL
#   api_key_env: "LITELLM_PROXY_KEY"
#   max_tokens: 4096

# -----------------------------------------------------------------------------
# 認証設定
# API/MCPアクセス制御
# -----------------------------------------------------------------------------
auth:
  enabled: false                  # 認証を有効にするか
  api_key_env: "COLONYFORGE_API_KEY"  # APIキーを格納する環境変数名

# -----------------------------------------------------------------------------
# サーバー設定
# REST APIサーバー
# -----------------------------------------------------------------------------
server:
  host: "0.0.0.0"                 # バインドホスト
  port: 8000                      # リッスンポート

# -----------------------------------------------------------------------------
# ロギング設定
# -----------------------------------------------------------------------------
logging:
  level: "INFO"                   # DEBUG | INFO | WARNING | ERROR
  events_max_file_size_mb: 100    # イベントログファイル最大サイズ（MB）

# =============================================================================
# エージェント設定
# Beekeeper / Queen Bee / Worker Bee の動作制御
# 各エージェントは個別のLLM設定を持てる（未設定時はグローバルllm設定を使用）
#
# =============================================================================
# プロンプトYAML設定（別ファイル）
# =============================================================================
# 各エージェントのシステムプロンプトはYAMLファイルで管理します。
# 読み込み優先順位:
#   1. Vault/hives/{hive_id}/colonies/{colony_id}/ - Colony固有
#   2. Vault/hives/{hive_id}/                      - Hive全体のデフォルト
#   3. src/colonyforge/prompts/defaults/             - パッケージ内デフォルト
#   4. ハードコードフォールバック
#
# ファイル命名規則:
#   - beekeeper.yml            → Beekeeperプロンプト
#   - queen_bee.yml            → Queen Beeプロンプト
#   - {name}_worker_bee.yml    → Worker Beeプロンプト（名前付き）
#   - default_worker_bee.yml   → Worker Beeデフォルト
#
# 例: Vault/hives/my-hive/colonies/backend/queen_bee.yml
#   name: backend-queen
#   prompt:
#     system: "あなたはバックエンド開発専門のQueen Beeです。"
#   max_workers: 3
#   task_assignment_strategy: priority
#
# 以下のagentsセクションは「ガバナンス設定」（LLM設定、タイムアウト等）を制御し、
# プロンプト内容はYAMLファイルで個別に管理する構成です。
# =============================================================================
agents:
  # ---------------------------------------------------------------------------
  # Beekeeper（養蜂家）
  # ユーザーとの対話窓口、Colony間の調整役
  # プロンプト: Vault/hives/{hive_id}/beekeeper.yml
  # ---------------------------------------------------------------------------
  beekeeper:
    enabled: true                     # Beekeeperを有効にするか
    max_colonies: 10                  # 管理可能な最大Colony数
    session_timeout_minutes: 60       # セッションタイムアウト（分）
    # llm:                            # Beekeeper専用LLM設定（オプション）
    #   provider: "openai"            # グローバル設定を上書きする場合のみ指定
    #   model: "gpt-4o"               # 高性能モデルで複雑な調整を担当
    #   temperature: 0.3              # やや創造的な応答
    #   api_base: ""                  # カスタムAPIエンドポイント
    #   num_retries: 3                # リトライ回数

  # ---------------------------------------------------------------------------
  # Queen Bee（女王蜂）
  # Colonyの統括、Worker Beeへのタスク割り当て
  # プロンプト: Vault/hives/{hive_id}/colonies/{colony_id}/queen_bee.yml
  # ---------------------------------------------------------------------------
  queen_bee:
    enabled: true                     # Queen Beeを有効にするか
    max_workers_per_colony: 5         # Colony当たり最大Worker数
    task_assignment_strategy: "round_robin"
    # タスク割り当て戦略:
    #   round_robin    - 順番に割り当て
    #   priority       - 優先度ベース
    #   load_balanced  - 負荷分散
    # llm:                            # Queen Bee専用LLM設定（オプション）
    #   provider: "openai"
    #   model: "gpt-4o-mini"          # 高速・低コストモデルでタスク分解
    #   api_base: ""                  # カスタムAPIエンドポイント

  # ---------------------------------------------------------------------------
  # Worker Bee（働き蜂）
  # 実務を担当する専門エージェント
  # プロンプト: Vault/hives/{hive_id}/colonies/{colony_id}/{name}_worker_bee.yml
  # ---------------------------------------------------------------------------
  worker_bee:
    enabled: true                     # Worker Beeを有効にするか
    tool_timeout_seconds: 60          # ツール実行タイムアウト（秒）
    max_retries: 3                    # ツール実行リトライ回数
    trust_level_default: "standard"
    # デフォルト信頼レベル:
    #   untrusted  - 読み取り専用のみ許可
    #   limited    - 軽微な変更まで許可
    #   standard   - 通常操作許可（危険操作は確認必要）
    #   elevated   - 危険操作も自動承認
    #   full       - 全操作自動承認
    # llm:                            # Worker Bee専用LLM設定（オプション）
    #   provider: "anthropic"         # 異なるプロバイダーも使用可能
    #   model: "claude-3-5-sonnet"
    #   api_key_env: "ANTHROPIC_API_KEY"
    #   api_base: ""                  # カスタムAPIエンドポイント
    #   fallback_models:              # フォールバック先
    #     - "openai/gpt-4o-mini"

# =============================================================================
# Sentinel Hornet設定（M2-0追加）
# Hive内監視エージェント — 異常検出と自動停止
# =============================================================================
sentinel:
  enabled: true                       # Sentinel Hornetを有効にするか
  max_event_rate: 50                  # レートウィンドウ内の最大イベント数
  rate_window_seconds: 60             # レート計測ウィンドウ（秒）
  max_loop_count: 5                   # ループ検出閾値（同一パターンの繰り返し回数）
  max_cost: 100.0                     # 最大コスト（ドル）
  auto_suspend: true                  # critical時にColonyを自動一時停止するか

# =============================================================================
# Swarming Protocol設定（M3-2追加）
# タスク特性に応じた適応的Colony編成
# =============================================================================
swarming:
  enabled: true                       # Swarming Protocolを有効にするか
  default_template: "balanced"        # デフォルトテンプレート (speed/balanced/quality/recovery)

  # テンプレートカスタマイズ
  # 各テンプレートのパラメータを上書きできる
  templates:
    speed:
      min_workers: 1
      max_workers: 1
      guard_bee: false
      reviewer: false
      retry_limit: 1
    balanced:
      min_workers: 2
      max_workers: 3
      guard_bee: true
      reviewer: false
      retry_limit: 3
    quality:
      min_workers: 3
      max_workers: 5
      guard_bee: true
      reviewer: true
      retry_limit: 5
    recovery:
      min_workers: 1
      max_workers: 2
      guard_bee: true
      reviewer: false
      retry_limit: 5

# =============================================================================
# 衝突検出・解決設定
# 複数Colony間のリソース競合を管理
# =============================================================================
conflict:
  detection_enabled: true             # 衝突検出を有効にするか
  auto_resolve_low_severity: true     # 低深刻度の衝突を自動解決するか
  escalation_timeout_minutes: 30      # エスカレーションタイムアウト（分）

# =============================================================================
# Conference設定
# 複数Colonyが参加する会議機能
# =============================================================================
conference:
  enabled: true                       # Conferenceを有効にするか
  max_participants: 10                # 最大参加Colony数
  voting_timeout_minutes: 15          # 投票タイムアウト（分）
  quorum_percentage: 50               # 定足数（%）- 議決に必要な参加率
