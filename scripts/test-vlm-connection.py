#!/usr/bin/env python3
"""VLMæ¥ç¶šãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

devcontainerã‹ã‚‰Ollamaã¸ã®æ¥ç¶šã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ã€‚
"""

import asyncio
import sys
from pathlib import Path

# srcã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from hiveforge.vlm import OllamaClient


async def main():
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆURLï¼ˆDockerãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµŒç”±ï¼‰
    ollama_url = "http://hiveforge-ollama:11434"

    print(f"ğŸ”— Ollama URL: {ollama_url}")
    print()

    # OllamaClientæ¥ç¶šãƒ†ã‚¹ãƒˆ
    print("1ï¸âƒ£ OllamaClientæ¥ç¶šãƒ†ã‚¹ãƒˆ...")
    client = OllamaClient(base_url=ollama_url, timeout=60)

    if await client.is_available():
        print("   âœ… Ollamaæ¥ç¶šæˆåŠŸ")
    else:
        print("   âŒ Ollamaæ¥ç¶šå¤±æ•—")
        print("   â†’ docker compose -f docker-compose.vlm.yml up -d ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return 1

    # ãƒ¢ãƒ‡ãƒ«ä¸€è¦§
    print()
    print("2ï¸âƒ£ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«...")
    models = await client.list_models()
    for model in models:
        print(f"   - {model}")

    if not models:
        print("   âš ï¸  ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
        print("   â†’ ./scripts/vlm-env.sh setup ã§LLaVAã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
        return 1

    # VLMãƒ†ã‚¹ãƒˆï¼ˆç”»åƒãªã—ã€ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼‰
    print()
    print("3ï¸âƒ£ VLMãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ...")
    try:
        response = await client.analyze_image(
            image=b"",  # ãƒ€ãƒŸãƒ¼
            prompt="Say 'VLM is working!' in exactly those words.",
        )
        print(f"   å¿œç­”: {response.response[:100]}...")
        print(f"   å‡¦ç†æ™‚é–“: {response.total_duration_ms}ms")
        print("   âœ… ãƒ†ã‚­ã‚¹ãƒˆç”ŸæˆæˆåŠŸ")
    except Exception as e:
        print(f"   âŒ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå¤±æ•—: {e}")

    print()
    print("âœ¨ VLMç’°å¢ƒã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ï¼")
    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
